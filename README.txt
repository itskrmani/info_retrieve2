took reference for unicode from the paper 
    https://unicode.org/charts/PDF/U0900.pdf


the structure for the files should be:
    /data
        /hi
            /hi.text
    /hi
        /50
            ...
        /100
            ...
        /200
            ...
        /300
            ...
    /Word similarity
        hindi.txt
    /Ques1
        /cbow
        /fasttext
        /glove
        /sg
        cbow.py
        fasttest.py
        glove.py
        run_cbow.py
        run_fasttext.py
        run_glove.py
        run_sg.py
        sg.py
        and dat files, preprocessed datastructure used to run the models

    /Ques2
        /hindi_ner
            hi_dev.conll
            hi_train.conll
        indicbertmodel.py
    /Ques3
        /char_ngram
            char_bigram.py
            char_quadrigram.py
            char_trigram.py
            char_unigram.py
            and their corresponsidng text file which contain the actual output generated by the code
        /syllable
            syllable.py
            syllable_bigram.py
            syllable_trigram.py
            syllable_unigram.py
            and their corresponsidng text file which contain the actual output generated by the code
        /word_ngram   
            word_bigram.py
            word_trigram.py
            word_unigram.py
            and their corresponsidng text file which contain the actual output generated by the code
        /zipfian
            png files which shows the output 


Basic Idea to develope: 
    for ques 1, download the dataset from https://www.cfilt.iitb.ac.in/~diptesh/embeddings/monolingual/non-contextual/
    and dwonload the word similarity from https://drive.google.com/drive/folders/1VovzSE1-zXH0bKCar2M8peL4-62BSlZJ
    and then, i generate the cosine similarity  datastructure for CBOW, Skipgram, Fasttext and a dictionary for Glove whose format are in
cosim_of_modelname = {dim:{threshold:{(word1,word2,ground truth) : cosine similarity},..},..}
glove_vector_dimension = {only_words_thats_are_in hindi.txt : their corresponding vector}

these datastructure will be created in the preprocessed cmd 
and then if you want to generate the output for each model then there corresponding csv file will be created 
i generate 20 file for each model in the format of 
    Q1_dimension_modelname_threshold.csv    
        it contain the output in the form of 
            [Word1, Word2, Similarity Score, Ground Truth, label_correct, Label_similar]

        label_correct : our model actually predict correctly 
        label_similar : only if ground truth and cosim both are greator than or equal to thresold
        
            ex: 4.5  > 4     similar word pair
        after that model accuracy will print in the form of percentage
 
    Note :  if two word pairs if the ground truth shows 4.5  and threshold being 4 then the word pair is similar. 
    If your similarity value is 4.0 or above in this case then also the prediction is correct. If ground  truth is 2.6 it is dissimilar. 
    If in this case your similarity score is 4.0 or above then your prediction is wrong but if it is below 4.0 then the prediction is correct.


    for ques 2:
        download the bert model from https://indicnlp.ai4bharat.org/indic-bert/
        NER dataset from https://drive.google.com/file/d/1S5TOqIC37dxWCeQbA9VpplXOGAB7cIMV/view?usp=sharing


    the required output as shown below
                    precision    recall  f1-score   support

                    B-CORP       0.85      0.86      0.85      5198
                        B-CW       0.86      0.73      0.79      4069
                    B-GRP       0.89      0.82      0.86      5092
                    B-LOC       0.92      0.91      0.91      6162
                    B-PER       0.94      0.93      0.93      4652
                    B-PROD       0.87      0.85      0.86      5902
                    I-CORP       0.84      0.89      0.86      4518
                        I-CW       0.88      0.86      0.87      5047
                    I-GRP       0.92      0.92      0.92      8125
                    I-LOC       0.87      0.84      0.85      2400
                    I-PER       0.95      0.97      0.96      6491
                    I-PROD       0.84      0.88      0.86      3336
                        O       0.99      1.00      0.99    262384

                    accuracy                           0.97    323376
                macro avg       0.89      0.88      0.89    323376
                weighted avg       0.97      0.97      0.97    323376


    for ques 3:
        first download the dataset from https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/indiccorp/hi.tar.xz
        then extract the hi.txt file and then apply spilt cmd
            cmd =>  make split    // will split the files into 22 parts each of 1 gb to tackle the problem of main memory filled
    
    now data is ready, after that :
    
    i just seperate the all files with each other like for one model- file in one folder
    for each model i take sets of hindi letters and punchuations.
    punchuations will be remove at start, then convert the string of sentence into list of characters.
    then apply rules to remove the halant keyword in the manner of :

        curr_char      next_char              make_change
        consonant      halant                 add it to curr_char 
        consonant      matra                  add halant to curr_char
        consonant      consonant, vowels      add halant to curr_char and insert 'अ#' to next_char (for seperation i used 'अ#' )

    'अ#' : when it is used as consonant endings
    'अ' : when it is used stand-alone

    then i make the dictionary in which we have key : as a n_grams and value : as a frequencies
    
    for each million line it will print time so that one can track the run time of the model

    actually if one make dictionary for all the corpura then it is tough to make a dictionary which have top 100 n grams
    because it require too much main memory so for that i only take top 2500 n grams for each splitted file and then merge them
    so that finally we get top 2500 n gram, now one can easily find top 100 n gram from that.
     for output   token_ngram.txt  these files will generate in their corresponding folders.




if you understand above then you can easily apply below commands
    for Ques1 : 
        if dat files not present then run 
        cmd => make Q1_preprocessing

        if dat files present you can directly run the models
        cmd => make runq1_models

    for Ques3:
        to generate the top 100 char n grams
        cmd => make runq3_char_ngram

        to generate the top 100 word n grams
        cmd => make runq3_word_ngram

        to generate the top 100 syllable
        cmd => make runq3_syllable

   


